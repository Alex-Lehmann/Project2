---
title: ""
pagetitle: "ALChapter5"
author: ""
date: ""
output: html_document
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readxl)
library(tidyverse)
library(tidytext)
library(gtools)
```

## Chapter 5-1: Loading Data
```{r load, message = FALSE}
# Translated responses =========================================================
responses <- read_xlsx("../masterResponse.xlsx") %>%
  select(-response) %>%
  rename(
    internal_id = c_1,
    language = c_2,
    question = column,
    response = english
  ) %>%
  select(-`...1`)

# Augment with survey MC responses =============================================
# Load -------------------------------------------------------------------------
survey <- read_xlsx("../Redacted FAB_Project_raw_data_Clean EXCEL Dec.23.xlsx")
colnames(survey) = c(
                     "internal_id",
                     "language",
                     "status",
                     "date",
                     "region",
                     "area1",
                     "area2",
                     "area3",
                     "area4",
                     "area5",
                     "cra_tenure",
                     "fab_tenure",
                     "tools_training",
                     "c_14",
                     "corporate_knowledge",
                     "innovation",
                     "share_ideas",
                     "heard",
                     "know_clients",
                     "clients_understand",
                     "comm_preference",
                     "c_22",
                     "timely_response",
                     "prevents_service1",
                     "prevents_service2",
                     "prevents_service3",
                     "prevents_service4",
                     "prevents_service5",
                     "prevents_service6",
                     "c_30",
                     "prevents_service7",
                     "importance1",
                     "importance2",
                     "importance3",
                     "importance4",
                     "importance5",
                     "importance6",
                     "importance7",
                     "policy_improve",
                     "c_40",
                     "c_41",
                     "compliments",
                     "c_43",
                     "excellent_service",
                     "c_45",
                     "c_46",
                     "can_contact",
                     "c_48"
                   )
```

## Chapter 5-2: Cleaning 

The data contained categorical variables split across multiple columns and the long responses needed to be changed to lowercase and checked for strange characters that might not align with our sentiment lexicons.

```{r clean}
# Clean MC responses -----------------------------------------------------------
survey = survey %>%
  select(-c(language, status, date), -starts_with("c_")) %>%
  mutate(
    # Combine separated columns
    area = coalesce(area1, area2, area3, area4, area5),
    prevents_service = coalesce(
                         prevents_service1,
                         prevents_service2,
                         prevents_service3,
                         prevents_service4,
                         prevents_service5,
                         prevents_service6,
                         prevents_service7
                       ),
    .keep = "unused"
  )

# Augment text data ------------------------------------------------------------
responses = full_join(responses, survey, by = "internal_id")

clean <- responses %>%
  
  # Standardize word representation --------------------------------------------
  mutate(
    response = str_to_lower(response),
    response = str_replace_all(response, "[^[[:alnum:]]']", " "),
    response = str_replace_all(response, "\\s{2,}", " ")
  ) %>%
  
  # Remove empty responses -----------------------------------------------------
  drop_na()
```

Next, we tokenized by word to prepare for sentiment mapping.

```{r tokenize}
tokens <- clean %>%
  
  # Convert responses to list-column for purrr ---------------------------------
  nest(cols = response) %>%
  rename(tokens = cols) %>%
  
  # Tokenize -------------------------------------------------------------------
  mutate(
    tokens = map(tokens, unnest_tokens,
               input = response, output = bigram,
               token = "ngrams", n = 2
             ),
    tokens = map(tokens, separate,
               col = bigram,
               into = c("preword", "word"),
               sep = " "
             )
  )
```

## Chapter 5-3: Map Sentiments

We mapped the NRC (restricted to positive/negative), Bing, and AFINN lexicons to our tokenized data. We used three lexicons to check for consistent effects across responses, and the AFINN lexicon allowed us to also investigate depth of sentiment. Negation was handled bigrammatically by negating the sentiment of a word whose preword is a negation word.

```{r sentiments, cache = TRUE}
# Helper function to map sentiments within a list-column =======================
map_sents <- function(tokens,
                      lexicon, 
                      neg_dict = c("no", "not", "isn't", "don't", "didn't")) {
  for (lex in lexicon) {
    cur_cols = colnames(tokens) # To update column names later
    
    # Map sentiments -----------------------------------------------------------
    tokens <- inner_join(tokens, get_sentiments(lex), by = "word")
    colnames(tokens)[ncol(tokens)] = "sentiment"
    
    # Convert sentiments to numeric --------------------------------------------
    tokens <- tokens %>%
      filter(sentiment %in% c("positive", "negative") | sentiment %in% -5:5) %>%
      mutate(
        sentiment = ifelse(sentiment == "positive", 1, sentiment),
        sentiment = ifelse(sentiment == "negative", -1, sentiment),
        sentiment = as.numeric(sentiment)
      ) %>%
      
      # Handle negation --------------------------------------------------------
      mutate(sentiment = ifelse(preword %in% neg_dict, -1*sentiment, sentiment))
    
    # Update column names ------------------------------------------------------
    colnames(tokens) <- c(cur_cols, lex)
  }
  
  tokens
}

# Map sentiments ===============================================================
sents <- mutate(tokens,
           sents = map(tokens, map_sents, lexicon = c("nrc", "bing", "afinn")),
           nrc = map_dbl(sents, ~ sum(.x$nrc)),
           bing = map_dbl(sents, ~ sum(.x$bing)),
           afinn = map_dbl(sents, ~ sum(.x$afinn))
         )
```

## Chapter 5-4: Analyze Sentiment

Here, we visualized the sentiment distributions across several categorical variables from the survey responses such as region, tenure, language, etc. We also generated visualizations of the sentiments by question to determine which questions, generally, saw higher or lower sentiment expressed.

```{r analyze_cats}
# Overall sentiments by worker properties ######################################
# Helper function ==============================================================
sents_by_cats <- function(var, sents) {
  sents %>%
    group_by(eval(as.name(var))) %>%
    summarize(across(any_of(c("nrc", "bing", "afinn")), mean), .groups = "drop") %>%
    rename(var = `eval(as.name(var))`)
}

# Get overall sentiments =======================================================
# By worker information --------------------------------------------------------
region_sents <- sents_by_cats("region", sents)
area_sents <- sents_by_cats("area", sents)
cra_tenure_sents <- sents_by_cats("cra_tenure", sents)
fab_tenure_sents <- sents_by_cats("fab_tenure", sents)
language_sents <- sents_by_cats("language", sents)

# Per question -----------------------------------------------------------------
question_sents <- sents_by_cats("question", sents)

# Visualize ====================================================================
# Helper function for barplotting ----------------------------------------------
sents_barplot <- function(sents, lex) {
  ggplot(sents, aes(x = var, y = eval(as.name(lex)))) +
    geom_bar(stat = "identity") +
    ylab(lex)
}

# Helper function for lineplotting ---------------------------------------------
sents_lineplot <- function(sents, lex) {
  ggplot(sents, aes(x = var, y = eval(as.name(lex)))) +
    geom_line() + 
    geom_hline(aes(yintercept = mean(sents$var)))
    ylab(lex)
}

# Generate plots ---------------------------------------------------------------
# Region
sents_barplot(region_sents, "nrc") + xlab("Region")
sents_barplot(region_sents, "bing") + xlab("Region")
sents_barplot(region_sents, "afinn") + xlab("Region")

# Work area
sents_barplot(area_sents, "nrc") + xlab("Area") + coord_flip()
sents_barplot(area_sents, "bing") + xlab("Area") + coord_flip()
sents_barplot(area_sents, "afinn") + xlab("Area") + coord_flip()

# Language
sents_barplot(language_sents, "nrc") + xlab("Language")
sents_barplot(language_sents, "bing") + xlab("Language")
sents_barplot(language_sents, "afinn") + xlab("Language")

# Question
sents_barplot(question_sents, "nrc") + xlab("Question")
sents_barplot(question_sents, "bing") + xlab("Question")
sents_barplot(question_sents, "afinn") + xlab("Question")

# CRA tenure
cra_tenure_sents %>%
  mutate(var = factor(var, 
                 levels = c(
                            "0 to 2 years",
                            "3 to 4 years",
                            "5 to 10 years",
                            "11 to 20 years",
                            "21 + years"
                          )
               )) %>%
  pivot_longer(!var, names_to = "lex", values_to = "sent") %>%
  ggplot(aes(x = var, y = sent, group = lex, color = lex)) +
  geom_line() +
  xlab("CRA Tenure")

# FAB tenure
fab_tenure_sents %>%
  mutate(var = factor(var, 
                 levels = c(
                            "0 to 2 years",
                            "3 to 4 years",
                            "5 to 10 years",
                            "11 to 20 years",
                            "21 + years"
                          )
               )) %>%
  pivot_longer(!var, names_to = "lex", values_to = "sent") %>%
  ggplot(aes(x = var, y = sent, group = lex, color = lex)) +
  geom_line() +
  xlab("FAB Tenure")
```

Sentiment analysis showed that the sentiment expressed in survey responses were generally quite positive, although certain segments of the population surveyed may display better or worse sentiment than others. In order to determine which of these differences are statistically significant, we used asymptotic permutation tests of hypothesis with the following hypotheses:  
$H_0$: The mean sentiment in Population 1 is the same as the mean sentiment in Population 2;  
$H_a$: The mean sentiment in Population 1 is higher/lower than the mean sentiment in Population 2.  
All tests were conducted at the $\alpha$=``0.05`` level of significance.

```{r analyze_perm_tests, cache = TRUE, message = FALSE}
# Function to run a permutation test on the sentiments =========================
# Mode 1 = >, mode 2 = <
perm_test <- function(df,
                     cats,
                     vals,
                     pop1,
                     pop2 = NULL,
                     fn = mean,
                     n = 1000,
                     mode = 1) {
  # Change labels to those required for the test -------------------------------
  if (is.null(pop2)) {
    test_df <- df %>%
      select(c(cats, vals)) %>%
      rename(cats = !!cats, vals = !!vals) %>%
      mutate(cats = ifelse(cats == pop1, "pop1", "pop2"))
  } else {
    test_df <- df %>%
      select(c(cats, vals)) %>%
      rename(cats = !!cats, vals = !!vals) %>%
      mutate(
        cats = ifelse(cats == pop1, "pop1", cats),
        cats = ifelse(cats == pop2, "pop2", cats)
      ) %>%
      filter(cats %in% c("pop1", "pop2"))
  }
  
  # Compute observed test statistic --------------------------------------------
  pop1_stat <- test_df %>%
    filter(cats == "pop1") %>%
    pull(vals) %>%
    fn()
  pop2_stat <- test_df %>%
    filter(cats == "pop2") %>%
    pull(vals) %>%
    fn()
  observed_stat <- pop1_stat - pop2_stat
  
  # Run test -------------------------------------------------------------------
  samples <- rep(NA, n)
  for (i in 1:n) {
    perm_df <- test_df %>%
      mutate(cats = permute(cats))
    stat1 <- perm_df %>%
      filter(cats == "pop1") %>%
      pull(vals) %>%
      fn()
    stat2 <- perm_df %>%
      filter(cats == "pop2") %>%
      pull(vals) %>%
      fn()
    samples[i] <- stat1 - stat2
  }
  
  # Compute p-value ------------------------------------------------------------
  if (mode == 1) p <- sum(samples >= observed_stat) / n
  else p <- sum(samples <= observed_stat) / n
  
  return(p)
}

# Hypothesis tests =============================================================
# Mean sentiment in Knowledge and Research area is higher than the rest --------
set.seed(101061620)
perm_test(sents, "area", "afinn", "Knowledge and Research", n = 10000)

# Mean sentiment among French speakers is lower than English -------------------
perm_test(sents, "language", "nrc", "FR", n = 10000, mode = 2)
```

We see that according to the AFINN lexicon, survey responses on average displayed a significantly more positive sentiment than those of other areas. However, according to the NRC lexicon, there is insufficient evidence to conclude that French-language surveys expressed, on average, more negative sentiments than English-language surveys.
