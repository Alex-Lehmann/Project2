system("./SetGit.sh")
# Load data from store
## View your working directory
getwd()
## Read in the idioms from file
storeIdioms <- readRDS("idiomsRead.rds")
## Clean the idiom list
for(currentLetter in 1:length(storeIdioms)) {
storeIdioms[[currentLetter]]$expression <- tolower(storeIdioms[[currentLetter]]$expression)
storeIdioms[[currentLetter]]$expression <- str_replace_all(storeIdioms[[currentLetter]]$expression, pattern = " \\(.*?\\)", "")
storeIdioms[[currentLetter]]$definition <- tolower(storeIdioms[[currentLetter]]$definition)
}
library(rvest)
library(stringr)
library(tidyverse)
library(sentimentr)
## Clean the idiom list
for(currentLetter in 1:length(storeIdioms)) {
storeIdioms[[currentLetter]]$expression <- tolower(storeIdioms[[currentLetter]]$expression)
storeIdioms[[currentLetter]]$expression <- str_replace_all(storeIdioms[[currentLetter]]$expression, pattern = " \\(.*?\\)", "")
storeIdioms[[currentLetter]]$definition <- tolower(storeIdioms[[currentLetter]]$definition)
}
# Sentiment Analysis
## Initialize for sentiment analysis
#install.packages("sentimentr")
library(sentimentr)
## Process sentiment
### Create sentiment store tibble
sTibble <- data.frame(expression = c(), word_count = c(), sd = c(),
ave_sentiment = c())
### Process sentiments
for(sTibbleIndex in 1:length(storeIdioms)) {
storeIdioms[[sTibbleIndex]] %>%
get_sentences() %>%
sentiment_by(., by = "expression") %>%
rbind(sTibble,.) -> sTibble
}
### Create sentiment tibble with average results
rsTibble <- tibble(expression = sTibble$expression,
sentimentF = sTibble$ave_sentiment)
### Norm the sentiments
maxSrst <- max(rsTibble$sentimentF)
minSrst <- min(rsTibble$sentimentF)
rsTibble$sentimentF[rsTibble$sentimentF > 0] <-
round(4 * rsTibble$sentimentF[rsTibble$sentimentF > 0] / maxSrst) / 4
rsTibble$sentimentF[rsTibble$sentimentF < 0] <-
round(4 * rsTibble$sentimentF[rsTibble$sentimentF < 0] / minSrst) / -4
sentimentrTibble <- rsTibble[!duplicated(rsTibble$expression),]
sentimentrTibble
# Emotional Analysis
aTibble <- data.frame()
## For emotional processing we merge the definition and expression
for(aTibbleIndex in 1:length(storeIdioms)) {
aTibble <- rbind(aTibble,
tibble(expression = storeIdioms[[aTibbleIndex]]$expression,
defEx = paste(storeIdioms[[aTibbleIndex]]$expression,
storeIdioms[[aTibbleIndex]]$definition,
sep = ", ")))
}
## We will get the sentences for emotional processing but will amalgamate sentences first to avoid multiple entries
emotionsTibble <- emotion(get_sentences(gsub("\\!",";",gsub("\\.",";",aTibble$defEx,))))
## Separate emotional declarations into logical vectors
emotionsNoted <- c("anger", "anger_negated", "anticipation", "anticipation_negated",
"disgust", "disgust_negated", "fear", "fear_negated",
"joy", "joy_negated", "sadness", "sadness_negated",
"surprise", "surprise_negated", "trust", "trust_negated")
anger = emotionsTibble$emotion_type == emotionsNoted[1]
anger_negated = emotionsTibble$emotion_type == emotionsNoted[2]
anticipation = emotionsTibble$emotion_type == emotionsNoted[3]
anticipation_negated = emotionsTibble$emotion_type == emotionsNoted[4]
disgust = emotionsTibble$emotion_type == emotionsNoted[5]
disgust_negated = emotionsTibble$emotion_type == emotionsNoted[6]
fear = emotionsTibble$emotion_type == emotionsNoted[7]
fear_negated = emotionsTibble$emotion_type == emotionsNoted[8]
joy = emotionsTibble$emotion_type == emotionsNoted[9]
joy_negated = emotionsTibble$emotion_type == emotionsNoted[10]
sadness = emotionsTibble$emotion_type == emotionsNoted[11]
sadness_negated = emotionsTibble$emotion_type == emotionsNoted[12]
surprise = emotionsTibble$emotion_type == emotionsNoted[13]
surprise_negated = emotionsTibble$emotion_type == emotionsNoted[14]
trust = emotionsTibble$emotion_type == emotionsNoted[15]
trust_negated = emotionsTibble$emotion_type == emotionsNoted[16]
## Create and emotions logic frame
emotionsLogic <- data.frame(
anger,
anger_negated,
anticipation,
anticipation_negated,
disgust,
disgust_negated,
fear,
fear_negated,
joy,
joy_negated,
sadness,
sadness_negated,
surprise,
surprise_negated,
trust,
trust_negated
)
## Combine the emotions into their original categories by collapsing negations and finding maximal emotional hits
strictEmotionsNoted <- c("anger", "anticipation",
"disgust", "fear",
"joy", "sadness",
"surprise", "trust")
emotionsSelectIndex <- 1:length(strictEmotionsNoted)
token <- c()
d <- c()
for(elementIndex in 1:max(emotionsTibble$element_id)){
overarchingLogic <- (emotionsTibble$element_id == elementIndex)
currentAnger <- emotionsTibble$emotion[overarchingLogic&emotionsLogic$anger] -
emotionsTibble$emotion[overarchingLogic&emotionsLogic$anger_negated]
currentAnticipation <- emotionsTibble$emotion[overarchingLogic&emotionsLogic$anticipation] -
emotionsTibble$emotion[overarchingLogic&emotionsLogic$anticipation_negated]
currentDisgust <- emotionsTibble$emotion[overarchingLogic&emotionsLogic$disgust] -
emotionsTibble$emotion[overarchingLogic&emotionsLogic$disgust_negated]
currentFear <- emotionsTibble$emotion[overarchingLogic&emotionsLogic$fear] -
emotionsTibble$emotion[overarchingLogic&emotionsLogic$fear_negated]
currentJoy <- emotionsTibble$emotion[overarchingLogic&emotionsLogic$joy] -
emotionsTibble$emotion[overarchingLogic&emotionsLogic$joy_negated]
currentSadness <- emotionsTibble$emotion[overarchingLogic&emotionsLogic$sadness] -
emotionsTibble$emotion[overarchingLogic&emotionsLogic$sadness_negated]
currentSurprise <- emotionsTibble$emotion[overarchingLogic&emotionsLogic$surprise] -
emotionsTibble$emotion[overarchingLogic&emotionsLogic$surprise_negated]
currentTrust <- emotionsTibble$emotion[overarchingLogic&emotionsLogic$trust] -
emotionsTibble$emotion[overarchingLogic&emotionsLogic$trust_negated]
currentVector <- c(currentAnger, currentAnticipation, currentDisgust, currentFear,
currentJoy, currentSadness, currentSurprise, currentTrust)
currentMax <- max(currentVector)
length(emotionsSelectIndex[currentVector==currentMax])
if(currentMax > 0){
token <- c(token,rep(aTibble$expression[elementIndex],
sum(currentVector==currentMax)))
d <- c(d, strictEmotionsNoted[emotionsSelectIndex[currentVector==currentMax]])
} else {
token <- c(token, aTibble$expression[elementIndex])
d <- c(d, "")
}
}
## Creat summary emotial table
#lexicon::hash_nrc_emotions
overallEmotionTibble <- tibble(token,
emotion = d)
finalEmotionTibble <- overallEmotionTibble[!duplicated(paste(overallEmotionTibble$token, overallEmotionTibble$emotion)),]
## View
View(finalEmotionTibble)
shiny::runApp('dataVisualization')
runApp('dataVisualization')
runApp('dataVisualization')
storeIdioms
getwd()
paste(dirIn,fileIn,sep="/")
dirIn <- "/Users/johnbrooks/Desktop/Course Work/STAT5702/Project2"
fileIn <- "Redacted FAB_Project_raw_data_Clean EXCEL Dec.23"
paste(dirIn,fileIn,sep="/")
# Initialize libraries
library(xlsx)
# Create Path
pathIn <- paste(dirIn,fileIn,".xlsx",sep="/")
# Initialize libraries
library(xlsx)
# Read in file
dirIn <- "/Users/johnbrooks/Desktop/Course Work/STAT5702/Project2"
fileIn <- "Redacted FAB_Project_raw_data_Clean EXCEL Dec.23"
# Create Path
pathIn <- paste(dirIn,fileIn,".xlsx",sep="/")
# Read in data
rawData <- read.xlsx(pathIn)
# Initialize libraries
library(xlsx)
# Read in file
dirIn <- "/Users/johnbrooks/Desktop/Course Work/STAT5702/Project2"
fileIn <- "Redacted FAB_Project_raw_data_Clean EXCEL Dec.23"
# Create Path
pathIn <- paste(dirIn,fileIn,".xlsx",sep="/")
# Read in data
rawData <- read.xlsx(pathIn,1)
# Create Path
pathIn <- paste(dirIn,"/",fileIn,".xlsx",sep="")
# Read in data
rawData <- read.xlsx(pathIn,1)
View()
View(rawData)
ncol(rawData)
names(rawData)
# Process columns
cleanData <- rawData
columnNamesStore <- names(rawData)
library(tidyr)
columnNamesStore <- names(rawData)
cleanData <- rawData
names(cleanData) <- 1:ncol(cleanData)
cleanData
View(cleanData)
nonCatColumns <- c(14,
22,
30,
40,
41,
43,
45,
46,
48)
nonCatColumns
cleanData[,nonCatColumns]
View(cleanData[,nonCatColumns])
View(cleanData[,c(1,nonCatColumns)])
columnNamesStore
columnNamesStore[1]
columnNamesStore[2]
# Scrape free text columns
View(cleanData[,c(1,2,nonCatColumns)])
# Scrape free text columns
View(cleanData[,c(1,2,nonCatColumns)])
# Scrape free text columns
procData <- cleanData[,c(1,2,nonCatColumns)]
?filter()
# Pivot the data
pivtData <- procData %>%
filter(2 == EN)
names(cleanData) <- Paste("c",1:ncol(cleanData),sep="_")
names(cleanData) <- paste("c",1:ncol(cleanData),sep="_")
cleanData
nonCatColumns <- c(14,
22,
30,
40,
41,
43,
45,
46,
48)
# Scrape free text columns
procData <- cleanData[,c(1,2,nonCatColumns)]
procData
######### Translate here
pivtData <- procData %>%
filter(c_2 == "EN")
names(procData)
######### Translate here
pivtData <- procData %>%
filter("c_2" == "EN")
pivtData
library(dplyr)
# Read in file
dirIn <- "/Users/johnbrooks/Desktop/Course Work/STAT5702/Project2"
######### Translate here
pivtData <- procData %>%
filter("c_2" == "EN")
######### Translate here
pivtData <- procData %>%
filter(c_2 == "EN")
pivtData
pivtData$c_2
names(pivtData)
newNames <- names(procData)
newNames
selectNames  <- newNames[3:length(newNames)]
selectNames
a
######### Translate here
pivtData <- procData %>%
filter(c_2 == "EN") %>%
select(selectNames)
procData <- cleanData[,c(1,2,nonCatColumns)]
newNames <- names(procData)
selectNames  <- newNames[c(1,3:length(newNames))]
######### Translate here
pivtData <- procData %>%
filter(c_2 == "EN") %>%
select(selectNames)
pivtData
View(pivtData)
# Pivot the data
pivtData <- procData %>%
# Take out french
filter(c_2 == "EN") %>%
# Select only those columns with the ID and the phrases
select(c(1,selectNames)) %>%
# Pivot the data
pivot_longer(selectNames)
selectNames  <- newNames[c(3:length(newNames))]
# Pivot the data
pivtData <- procData %>%
# Take out french
filter(c_2 == "EN") %>%
# Select only those columns with the ID and the phrases
select(c(1,selectNames)) %>%
# Pivot the data
pivot_longer(selectNames)
View(pivtData)
pivtData <- procData %>%
# Take out french
filter(c_2 == "EN") %>%
# Select only those columns with the ID and the phrases
select(c(1,2,selectNames)) %>%
# Pivot the data to be cataloged by ID and question index
pivot_longer(selectNames)
View(pivtData)
# Prepare data for translation
forTranslation <- pivtData %>%
# Take out french
filter(c_2 == "FR")
# Pivot the data
pivtData <- procData %>%
# Select only those columns with the ID and the phrases
select(c(1,2,selectNames)) %>%
# Pivot the data to be cataloged by ID and question index
pivot_longer(selectNames)
# Prepare data for translation
forTranslation <- pivtData %>%
# Take out french
filter(c_2 == "FR")
forTranslation
# Prepare data for translation
forProcessEng <- pivtData %>%
# Take out french
filter(c_2 == "EN")
forProcessEng
# Pivot the data
pivtData <- procData %>%
# Select only those columns with the ID and the phrases
select(c(1,2,selectNames)) %>%
# Pivot the data to be cataloged by ID and question index
pivot_longer(selectNames,"response")
# Prepare data for translation
forTranslation <- pivtData %>%
# Take out french
filter(c_2 == "FR")
forTranslation
?pivot_longer()
# Pivot the data
pivtData <- procData %>%
# Select only those columns with the ID and the phrases
select(c(1,2,selectNames)) %>%
# Pivot the data to be cataloged by ID and question index
pivot_longer(selectNames,c("column","response"))
# Prepare data for translation
forTranslation <- pivtData %>%
# Take out french
filter(c_2 == "FR")
# Pivot the data
pivtData <- procData %>%
# Select only those columns with the ID and the phrases
select(c(1,2,selectNames)) %>%
# Pivot the data to be cataloged by ID and question index
pivot_longer(selectNames,names_to = "column",values_to = "response")
# Prepare data for translation
forTranslation <- pivtData %>%
# Take out french
filter(c_2 == "FR")
forTranslation
# Prepare data for translation
forProcessEng <- pivtData %>%
# Take out french
filter(c_2 == "EN")
# Initialize libraries
library(xlsx)
library(tidyr)
library(tidytext)
library(tidyverse)
library(dplyr)
?unnest_tokens()
forProcessEng
# Prepare data for translation
forProcessEng <- pivtData %>%
# Take out french
filter(c_2 == "EN") %>%
# Get the responses into words
unnest_tokens(word, response)
forProcessEng
# Prepare data for translation
forProcessEng <- pivtData %>%
# Take out french
filter(c_2 == "EN") %>%
# Get the responses into words
unnest_tokens(word, response) %>%
# Get the unique words
unique()
forProcessEng
# Prepare data for translation
forProcessEng <- pivtData %>%
# Take out french
filter(c_2 == "EN") %>%
# Get the responses into words
unnest_tokens(word, response) %>%
# Select the words column
select(c("word")) %>%
# Get the unique words
unique()
forProcessEng
## Words list
install.packages("qdapDictionaries")
## Words list
# install.packages("qdapDictionaries")
library(qdapDictionaries)
# load the dictionary
data(DICTIONARY)
# load the dictionary
dictionaryForUse <- data(DICTIONARY)
dictionaryForUse
library(qdapDictionaries)
qdapDictionaries::DICTIONARY
# load the dictionary
wordVector <- qdapDictionaries::DICTIONARY$word
c("a","b") %in% "back"
c("a","b") %in% "b"
forProcessEng
# load the dictionary
wordVector <- qdapDictionaries::DICTIONARY$word
forProcessEng(!(forProcessEng %in% wordVector))
forProcessEng[!(forProcessEng %in% wordVector)]
wordVector
sum(wordVector == "on")
forProcessEng[1]
forProcessEng[1,]
forProcessEng[1,1]
forProcessEng[2,1]
as.character(forProcessEng[2,1])
lengthProc <- nrow(forProcessEng)
lengthProc
isWord <- rep(FALSE,lengthProc)
isWord
wordVector <- qdapDictionaries::DICTIONARY$word
lengthProc <- nrow(forProcessEng)
isWord <- rep(FALSE,lengthProc)
for(currentIndex in 1:nrow(forProcessEng)) {
isWord[currentIndex] <- as.character(forProcessEng[currentIndex,1]) %in%
wordVector
}
isWord
wordVector[!isWord]
wordVector
"bid" %in% wordVector
isWord <- rep(FALSE,lengthProc)
isWord
for(currentIndex in 1:nrow(forProcessEng)) {
isWord[currentIndex] <- as.character(forProcessEng[currentIndex,1]) %in%
wordVector
}
forProcessEng
forProcessEng[!isWord,1]
wordfile <- read.csv("/Users/johnbrooks/Desktop/Course Work/STAT5702/Project2/words.txt",sep="\n")
wordfile
lengthProc <- nrow(forProcessEng)
isWord <- rep(FALSE,lengthProc)
for(currentIndex in 1:nrow(forProcessEng)) {
isWord[currentIndex] <- as.character(forProcessEng[currentIndex,1]) %in%
wordfile$X2
}
# See non-words
forProcessEng[!isWord,1]
wordfile$X2
"strong" %in% wordfile$X2
wordsList <- tolower(wordfile$X2)
wordsList <- tolower(wordfile$X2)
lengthProc <- nrow(forProcessEng)
isWord <- rep(FALSE,lengthProc)
for(currentIndex in 1:nrow(forProcessEng)) {
isWord[currentIndex] <- as.character(forProcessEng[currentIndex,1]) %in%
wordsList
}
forProcessEng[!isWord,1]
View(forProcessEng[!isWord,1])
